{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Source: https://www.thepythoncode.com/article/stock-price-prediction-in-python-using-tensorflow-2-and-keras \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from yahoo_fin import stock_info as si\n",
    "from collections import deque\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(314)\n",
    "tf.random.set_seed(314)\n",
    "random.seed(314)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_in_unison(a, b):\n",
    "    # shuffle two arrays in the same way\n",
    "    state = np.random.get_state()\n",
    "    np.random.shuffle(a)\n",
    "    np.random.set_state(state)\n",
    "    np.random.shuffle(b)\n",
    "\n",
    "def load_data(ticker, n_steps=50, scale=True, shuffle=True, lookup_step=1, split_by_date=True,\n",
    "                test_size=0.2, feature_columns=['adjclose', 'volume', 'open', 'high', 'low']):\n",
    "    \"\"\"\n",
    "    Loads data from Yahoo Finance source, as well as scaling, shuffling, normalizing and splitting.\n",
    "    Params:\n",
    "        ticker (str/pd.DataFrame): the ticker you want to load, examples include AAPL, TESL, etc.\n",
    "        n_steps (int): the historical sequence length (i.e window size) used to predict, default is 50\n",
    "        scale (bool): whether to scale prices from 0 to 1, default is True\n",
    "        shuffle (bool): whether to shuffle the dataset (both training & testing), default is True\n",
    "        lookup_step (int): the future lookup step to predict, default is 1 (e.g next day)\n",
    "        split_by_date (bool): whether we split the dataset into training/testing by date, setting it \n",
    "            to False will split datasets in a random way\n",
    "        test_size (float): ratio for test data, default is 0.2 (20% testing data)\n",
    "        feature_columns (list): the list of features to use to feed into the model, default is everything grabbed from yahoo_fin\n",
    "    \"\"\"\n",
    "    # see if ticker is already a loaded stock from yahoo finance\n",
    "    if isinstance(ticker, str):\n",
    "        # load it from yahoo_fin library\n",
    "        df = si.get_data(ticker)\n",
    "    elif isinstance(ticker, pd.DataFrame):\n",
    "        # already loaded, use it directly\n",
    "        df = ticker\n",
    "    else:\n",
    "        raise TypeError(\"ticker can be either a str or a `pd.DataFrame` instances\")\n",
    "    # this will contain all the elements we want to return from this function\n",
    "    result = {}\n",
    "    # we will also return the original dataframe itself\n",
    "    result['df'] = df.copy()\n",
    "    # make sure that the passed feature_columns exist in the dataframe\n",
    "    for col in feature_columns:\n",
    "        assert col in df.columns, f\"'{col}' does not exist in the dataframe.\"\n",
    "    # add date as a column\n",
    "    if \"date\" not in df.columns:\n",
    "        df[\"date\"] = df.index\n",
    "    if scale:\n",
    "        column_scaler = {}\n",
    "        # scale the data (prices) from 0 to 1\n",
    "        for column in feature_columns:\n",
    "            scaler = preprocessing.MinMaxScaler()\n",
    "            df[column] = scaler.fit_transform(np.expand_dims(df[column].values, axis=1))\n",
    "            column_scaler[column] = scaler\n",
    "        # add the MinMaxScaler instances to the result returned\n",
    "        result[\"column_scaler\"] = column_scaler\n",
    "    # add the target column (label) by shifting by `lookup_step`\n",
    "    df['future'] = df['adjclose'].shift(-lookup_step)\n",
    "    # last `lookup_step` columns contains NaN in future column\n",
    "    # get them before droping NaNs\n",
    "    last_sequence = np.array(df[feature_columns].tail(lookup_step))\n",
    "    # drop NaNs\n",
    "    df.dropna(inplace=True)\n",
    "    sequence_data = []\n",
    "    sequences = deque(maxlen=n_steps)\n",
    "    for entry, target in zip(df[feature_columns + [\"date\"]].values, df['future'].values):\n",
    "        sequences.append(entry)\n",
    "        if len(sequences) == n_steps:\n",
    "            sequence_data.append([np.array(sequences), target])\n",
    "    # get the last sequence by appending the last `n_step` sequence with `lookup_step` sequence\n",
    "    # for instance, if n_steps=50 and lookup_step=10, last_sequence should be of 60 (that is 50+10) length\n",
    "    # this last_sequence will be used to predict future stock prices that are not available in the dataset\n",
    "    last_sequence = list([s[:len(feature_columns)] for s in sequences]) + list(last_sequence)\n",
    "    last_sequence = np.array(last_sequence).astype(np.float32)\n",
    "    # add to result\n",
    "    result['last_sequence'] = last_sequence\n",
    "    # construct the X's and y's\n",
    "    X, y = [], []\n",
    "    for seq, target in sequence_data:\n",
    "        X.append(seq)\n",
    "        y.append(target)\n",
    "    # convert to numpy arrays\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    if split_by_date:\n",
    "        # split the dataset into training & testing sets by date (not randomly splitting)\n",
    "        train_samples = int((1 - test_size) * len(X))\n",
    "        result[\"X_train\"] = X[:train_samples]\n",
    "        result[\"y_train\"] = y[:train_samples]\n",
    "        result[\"X_test\"]  = X[train_samples:]\n",
    "        result[\"y_test\"]  = y[train_samples:]\n",
    "        if shuffle:\n",
    "            # shuffle the datasets for training (if shuffle parameter is set)\n",
    "            shuffle_in_unison(result[\"X_train\"], result[\"y_train\"])\n",
    "            shuffle_in_unison(result[\"X_test\"], result[\"y_test\"])\n",
    "    else:    \n",
    "        # split the dataset randomly\n",
    "        result[\"X_train\"], result[\"X_test\"], result[\"y_train\"], result[\"y_test\"] = train_test_split(X, y, \n",
    "                                                                                test_size=test_size, shuffle=shuffle)\n",
    "    # get the list of test set dates\n",
    "    dates = result[\"X_test\"][:, -1, -1]\n",
    "    # retrieve test features from the original dataframe\n",
    "    result[\"test_df\"] = result[\"df\"].loc[dates]\n",
    "    # remove duplicated dates in the testing dataframe\n",
    "    result[\"test_df\"] = result[\"test_df\"][~result[\"test_df\"].index.duplicated(keep='first')]\n",
    "    # remove dates from the training/testing sets & convert to float32\n",
    "    result[\"X_train\"] = result[\"X_train\"][:, :, :len(feature_columns)].astype(np.float32)\n",
    "    result[\"X_test\"] = result[\"X_test\"][:, :, :len(feature_columns)].astype(np.float32)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(sequence_length, n_features, units=256, cell=LSTM, n_layers=2, dropout=0.3,\n",
    "                loss=\"mean_absolute_error\", optimizer=\"rmsprop\", bidirectional=False):\n",
    "    model = Sequential()\n",
    "    for i in range(n_layers):\n",
    "        if i == 0:\n",
    "            # first layer\n",
    "            if bidirectional:\n",
    "                model.add(Bidirectional(cell(units, return_sequences=True), batch_input_shape=(None, sequence_length, n_features)))\n",
    "            else:\n",
    "                model.add(cell(units, return_sequences=True, batch_input_shape=(None, sequence_length, n_features)))\n",
    "        elif i == n_layers - 1:\n",
    "            # last layer\n",
    "            if bidirectional:\n",
    "                model.add(Bidirectional(cell(units, return_sequences=False)))\n",
    "            else:\n",
    "                model.add(cell(units, return_sequences=False))\n",
    "        else:\n",
    "            # hidden layers\n",
    "            if bidirectional:\n",
    "                model.add(Bidirectional(cell(units, return_sequences=True)))\n",
    "            else:\n",
    "                model.add(cell(units, return_sequences=True))\n",
    "        # add dropout after each layer\n",
    "        model.add(Dropout(dropout))\n",
    "    model.add(Dense(1, activation=\"linear\"))\n",
    "    model.compile(loss=loss, metrics=[\"mean_absolute_error\"], optimizer=optimizer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from tensorflow.keras.layers import LSTM\n",
    "\n",
    "# Window size or the sequence length\n",
    "N_STEPS = 50\n",
    "# Lookup step, 1 is the next day\n",
    "LOOKUP_STEP = 15\n",
    "# whether to scale feature columns & output price as well\n",
    "SCALE = True\n",
    "scale_str = f\"sc-{int(SCALE)}\"\n",
    "# whether to shuffle the dataset\n",
    "SHUFFLE = True\n",
    "shuffle_str = f\"sh-{int(SHUFFLE)}\"\n",
    "# whether to split the training/testing set by date\n",
    "SPLIT_BY_DATE = False\n",
    "split_by_date_str = f\"sbd-{int(SPLIT_BY_DATE)}\"\n",
    "# test ratio size, 0.2 is 20%\n",
    "TEST_SIZE = 0.2\n",
    "# features to use\n",
    "FEATURE_COLUMNS = [\"adjclose\", \"volume\", \"open\", \"high\", \"low\"]\n",
    "# date now\n",
    "date_now = time.strftime(\"%Y-%m-%d\")\n",
    "### model parameters\n",
    "N_LAYERS = 2\n",
    "# LSTM cell\n",
    "CELL = LSTM\n",
    "# 256 LSTM neurons\n",
    "UNITS = 256\n",
    "# 40% dropout\n",
    "DROPOUT = 0.4\n",
    "# whether to use bidirectional RNNs\n",
    "BIDIRECTIONAL = False\n",
    "### training parameters\n",
    "# mean absolute error loss\n",
    "# LOSS = \"mae\"\n",
    "# huber loss\n",
    "LOSS = \"huber_loss\"\n",
    "OPTIMIZER = \"adam\"\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 25\n",
    "# Amazon stock market\n",
    "ticker = \"AMZN\"\n",
    "ticker_data_filename = os.path.join(\"data\", f\"{ticker}_{date_now}.csv\")\n",
    "# model name to save, making it as unique as possible based on parameters\n",
    "model_name = f\"{date_now}_{ticker}-{shuffle_str}-{scale_str}-{split_by_date_str}-\\\n",
    "{LOSS}-{OPTIMIZER}-{CELL.__name__}-seq-{N_STEPS}-step-{LOOKUP_STEP}-layers-{N_LAYERS}-units-{UNITS}\"\n",
    "if BIDIRECTIONAL:\n",
    "    model_name += \"-b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create these folders if they does not exist\n",
    "if not os.path.isdir(\"results\"):\n",
    "    os.mkdir(\"results\")\n",
    "if not os.path.isdir(\"logs\"):\n",
    "    os.mkdir(\"logs\")\n",
    "if not os.path.isdir(\"data\"):\n",
    "    os.mkdir(\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      " 1/75 [..............................] - ETA: 0s - loss: 0.0210 - mean_absolute_error: 0.1093WARNING:tensorflow:From /home/randor/.local/lib/python3.8/site-packages/tensorflow/python/ops/summary_ops_v2.py:1277: stop (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n",
      "Instructions for updating:\n",
      "use `tf.profiler.experimental.stop` instead.\n",
      "75/75 [==============================] - ETA: 0s - loss: 0.0012 - mean_absolute_error: 0.0217\n",
      "Epoch 00001: val_loss improved from inf to 0.00018, saving model to results/2021-02-26_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
      "75/75 [==============================] - 22s 293ms/step - loss: 0.0012 - mean_absolute_error: 0.0217 - val_loss: 1.7755e-04 - val_mean_absolute_error: 0.0098\n",
      "Epoch 2/25\n",
      "75/75 [==============================] - ETA: 0s - loss: 4.0951e-04 - mean_absolute_error: 0.0139\n",
      "Epoch 00002: val_loss did not improve from 0.00018\n",
      "75/75 [==============================] - 23s 304ms/step - loss: 4.0951e-04 - mean_absolute_error: 0.0139 - val_loss: 8.8934e-04 - val_mean_absolute_error: 0.0299\n",
      "Epoch 3/25\n",
      "75/75 [==============================] - ETA: 0s - loss: 5.7879e-04 - mean_absolute_error: 0.0172\n",
      "Epoch 00003: val_loss improved from 0.00018 to 0.00017, saving model to results/2021-02-26_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
      "75/75 [==============================] - 23s 306ms/step - loss: 5.7879e-04 - mean_absolute_error: 0.0172 - val_loss: 1.7412e-04 - val_mean_absolute_error: 0.0089\n",
      "Epoch 4/25\n",
      "75/75 [==============================] - ETA: 0s - loss: 4.8266e-04 - mean_absolute_error: 0.0150\n",
      "Epoch 00004: val_loss improved from 0.00017 to 0.00013, saving model to results/2021-02-26_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
      "75/75 [==============================] - 23s 306ms/step - loss: 4.8266e-04 - mean_absolute_error: 0.0150 - val_loss: 1.2872e-04 - val_mean_absolute_error: 0.0079\n",
      "Epoch 5/25\n",
      "75/75 [==============================] - ETA: 0s - loss: 3.5740e-04 - mean_absolute_error: 0.0127\n",
      "Epoch 00005: val_loss improved from 0.00013 to 0.00012, saving model to results/2021-02-26_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
      "75/75 [==============================] - 23s 307ms/step - loss: 3.5740e-04 - mean_absolute_error: 0.0127 - val_loss: 1.1635e-04 - val_mean_absolute_error: 0.0082\n",
      "Epoch 6/25\n",
      "75/75 [==============================] - ETA: 0s - loss: 4.0047e-04 - mean_absolute_error: 0.0138\n",
      "Epoch 00006: val_loss improved from 0.00012 to 0.00011, saving model to results/2021-02-26_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
      "75/75 [==============================] - 24s 314ms/step - loss: 4.0047e-04 - mean_absolute_error: 0.0138 - val_loss: 1.0878e-04 - val_mean_absolute_error: 0.0072\n",
      "Epoch 7/25\n",
      "75/75 [==============================] - ETA: 0s - loss: 3.8456e-04 - mean_absolute_error: 0.0134\n",
      "Epoch 00007: val_loss did not improve from 0.00011\n",
      "75/75 [==============================] - 23s 306ms/step - loss: 3.8456e-04 - mean_absolute_error: 0.0134 - val_loss: 1.3566e-04 - val_mean_absolute_error: 0.0084\n",
      "Epoch 8/25\n",
      "75/75 [==============================] - ETA: 0s - loss: 3.6780e-04 - mean_absolute_error: 0.0129\n",
      "Epoch 00008: val_loss improved from 0.00011 to 0.00010, saving model to results/2021-02-26_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
      "75/75 [==============================] - 23s 305ms/step - loss: 3.6780e-04 - mean_absolute_error: 0.0129 - val_loss: 1.0393e-04 - val_mean_absolute_error: 0.0065\n",
      "Epoch 9/25\n",
      "75/75 [==============================] - ETA: 0s - loss: 4.0571e-04 - mean_absolute_error: 0.0135\n",
      "Epoch 00009: val_loss did not improve from 0.00010\n",
      "75/75 [==============================] - 23s 301ms/step - loss: 4.0571e-04 - mean_absolute_error: 0.0135 - val_loss: 1.7459e-04 - val_mean_absolute_error: 0.0103\n",
      "Epoch 10/25\n",
      "75/75 [==============================] - ETA: 0s - loss: 3.9208e-04 - mean_absolute_error: 0.0134\n",
      "Epoch 00010: val_loss did not improve from 0.00010\n",
      "75/75 [==============================] - 23s 306ms/step - loss: 3.9208e-04 - mean_absolute_error: 0.0134 - val_loss: 6.6204e-04 - val_mean_absolute_error: 0.0169\n",
      "Epoch 11/25\n",
      "75/75 [==============================] - ETA: 0s - loss: 6.1021e-04 - mean_absolute_error: 0.0170\n",
      "Epoch 00011: val_loss did not improve from 0.00010\n",
      "75/75 [==============================] - 23s 305ms/step - loss: 6.1021e-04 - mean_absolute_error: 0.0170 - val_loss: 1.1781e-04 - val_mean_absolute_error: 0.0075\n",
      "Epoch 12/25\n",
      "75/75 [==============================] - ETA: 0s - loss: 3.9879e-04 - mean_absolute_error: 0.0135\n",
      "Epoch 00012: val_loss did not improve from 0.00010\n",
      "75/75 [==============================] - 23s 301ms/step - loss: 3.9879e-04 - mean_absolute_error: 0.0135 - val_loss: 4.4298e-04 - val_mean_absolute_error: 0.0150\n",
      "Epoch 13/25\n",
      "75/75 [==============================] - ETA: 0s - loss: 3.5326e-04 - mean_absolute_error: 0.0138\n",
      "Epoch 00013: val_loss did not improve from 0.00010\n",
      "75/75 [==============================] - 23s 302ms/step - loss: 3.5326e-04 - mean_absolute_error: 0.0138 - val_loss: 1.2177e-04 - val_mean_absolute_error: 0.0073\n",
      "Epoch 14/25\n",
      "75/75 [==============================] - ETA: 0s - loss: 3.2831e-04 - mean_absolute_error: 0.0125\n",
      "Epoch 00014: val_loss did not improve from 0.00010\n",
      "75/75 [==============================] - 23s 302ms/step - loss: 3.2831e-04 - mean_absolute_error: 0.0125 - val_loss: 1.5661e-04 - val_mean_absolute_error: 0.0078\n",
      "Epoch 15/25\n",
      "75/75 [==============================] - ETA: 0s - loss: 3.5161e-04 - mean_absolute_error: 0.0131\n",
      "Epoch 00015: val_loss did not improve from 0.00010\n",
      "75/75 [==============================] - 23s 303ms/step - loss: 3.5161e-04 - mean_absolute_error: 0.0131 - val_loss: 1.4564e-04 - val_mean_absolute_error: 0.0091\n",
      "Epoch 16/25\n",
      "75/75 [==============================] - ETA: 0s - loss: 3.4715e-04 - mean_absolute_error: 0.0132\n",
      "Epoch 00016: val_loss did not improve from 0.00010\n",
      "75/75 [==============================] - 23s 302ms/step - loss: 3.4715e-04 - mean_absolute_error: 0.0132 - val_loss: 3.5958e-04 - val_mean_absolute_error: 0.0132\n",
      "Epoch 17/25\n",
      "75/75 [==============================] - ETA: 0s - loss: 3.3833e-04 - mean_absolute_error: 0.0128\n",
      "Epoch 00017: val_loss did not improve from 0.00010\n",
      "75/75 [==============================] - 23s 302ms/step - loss: 3.3833e-04 - mean_absolute_error: 0.0128 - val_loss: 1.3750e-04 - val_mean_absolute_error: 0.0086\n",
      "Epoch 18/25\n",
      "75/75 [==============================] - ETA: 0s - loss: 3.3654e-04 - mean_absolute_error: 0.0131\n",
      "Epoch 00018: val_loss did not improve from 0.00010\n",
      "75/75 [==============================] - 23s 300ms/step - loss: 3.3654e-04 - mean_absolute_error: 0.0131 - val_loss: 1.2790e-04 - val_mean_absolute_error: 0.0097\n",
      "Epoch 19/25\n",
      "75/75 [==============================] - ETA: 0s - loss: 2.9711e-04 - mean_absolute_error: 0.0124\n",
      "Epoch 00019: val_loss did not improve from 0.00010\n",
      "75/75 [==============================] - 22s 300ms/step - loss: 2.9711e-04 - mean_absolute_error: 0.0124 - val_loss: 1.0671e-04 - val_mean_absolute_error: 0.0070\n",
      "Epoch 20/25\n",
      "75/75 [==============================] - ETA: 0s - loss: 3.2639e-04 - mean_absolute_error: 0.0128\n",
      "Epoch 00020: val_loss did not improve from 0.00010\n",
      "75/75 [==============================] - 23s 300ms/step - loss: 3.2639e-04 - mean_absolute_error: 0.0128 - val_loss: 2.4467e-04 - val_mean_absolute_error: 0.0120\n",
      "Epoch 21/25\n",
      "75/75 [==============================] - ETA: 0s - loss: 3.3778e-04 - mean_absolute_error: 0.0131\n",
      "Epoch 00021: val_loss did not improve from 0.00010\n",
      "75/75 [==============================] - 23s 302ms/step - loss: 3.3778e-04 - mean_absolute_error: 0.0131 - val_loss: 1.2373e-04 - val_mean_absolute_error: 0.0080\n",
      "Epoch 22/25\n",
      "75/75 [==============================] - ETA: 0s - loss: 3.2576e-04 - mean_absolute_error: 0.0128\n",
      "Epoch 00022: val_loss did not improve from 0.00010\n",
      "75/75 [==============================] - 23s 304ms/step - loss: 3.2576e-04 - mean_absolute_error: 0.0128 - val_loss: 1.1086e-04 - val_mean_absolute_error: 0.0072\n",
      "Epoch 23/25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75/75 [==============================] - ETA: 0s - loss: 3.1764e-04 - mean_absolute_error: 0.0131\n",
      "Epoch 00023: val_loss did not improve from 0.00010\n",
      "75/75 [==============================] - 23s 300ms/step - loss: 3.1764e-04 - mean_absolute_error: 0.0131 - val_loss: 1.9997e-04 - val_mean_absolute_error: 0.0092\n",
      "Epoch 24/25\n",
      "75/75 [==============================] - ETA: 0s - loss: 3.0623e-04 - mean_absolute_error: 0.0124\n",
      "Epoch 00024: val_loss did not improve from 0.00010\n",
      "75/75 [==============================] - 23s 301ms/step - loss: 3.0623e-04 - mean_absolute_error: 0.0124 - val_loss: 1.5871e-04 - val_mean_absolute_error: 0.0091\n",
      "Epoch 25/25\n",
      "75/75 [==============================] - ETA: 0s - loss: 2.9258e-04 - mean_absolute_error: 0.0126\n",
      "Epoch 00025: val_loss did not improve from 0.00010\n",
      "75/75 [==============================] - 22s 300ms/step - loss: 2.9258e-04 - mean_absolute_error: 0.0126 - val_loss: 1.6866e-04 - val_mean_absolute_error: 0.0094\n"
     ]
    }
   ],
   "source": [
    "# load the data\n",
    "data = load_data(ticker, N_STEPS, scale=SCALE, split_by_date=SPLIT_BY_DATE, \n",
    "                shuffle=SHUFFLE, lookup_step=LOOKUP_STEP, test_size=TEST_SIZE, \n",
    "                feature_columns=FEATURE_COLUMNS)\n",
    "# save the dataframe\n",
    "data[\"df\"].to_csv(ticker_data_filename)\n",
    "# construct the model\n",
    "model = create_model(N_STEPS, len(FEATURE_COLUMNS), loss=LOSS, units=UNITS, cell=CELL, n_layers=N_LAYERS,\n",
    "                    dropout=DROPOUT, optimizer=OPTIMIZER, bidirectional=BIDIRECTIONAL)\n",
    "# some tensorflow callbacks\n",
    "checkpointer = ModelCheckpoint(os.path.join(\"results\", model_name + \".h5\"), save_weights_only=True, save_best_only=True, verbose=1)\n",
    "tensorboard = TensorBoard(log_dir=os.path.join(\"logs\", model_name))\n",
    "# train the model and save the weights whenever we see \n",
    "# a new optimal model using ModelCheckpoint\n",
    "history = model.fit(data[\"X_train\"], data[\"y_train\"],\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    epochs=EPOCHS,\n",
    "                    validation_data=(data[\"X_test\"], data[\"y_test\"]),\n",
    "                    callbacks=[checkpointer, tensorboard],\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_graph(test_df):\n",
    "    \"\"\"\n",
    "    This function plots true close price along with predicted close price\n",
    "    with blue and red colors respectively\n",
    "    \"\"\"\n",
    "    plt.plot(test_df[f'true_adjclose_{LOOKUP_STEP}'], c='b')\n",
    "    plt.plot(test_df[f'adjclose_{LOOKUP_STEP}'], c='r')\n",
    "    plt.xlabel(\"Days\")\n",
    "    plt.ylabel(\"Price\")\n",
    "    plt.legend([\"Actual Price\", \"Predicted Price\"])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_final_df(model, data):\n",
    "    \"\"\"\n",
    "    This function takes the `model` and `data` dict to \n",
    "    construct a final dataframe that includes the features along \n",
    "    with true and predicted prices of the testing dataset\n",
    "    \"\"\"\n",
    "    # if predicted future price is higher than the current, \n",
    "    # then calculate the true future price minus the current price, to get the buy profit\n",
    "    buy_profit  = lambda current, true_future, pred_future: true_future - current if pred_future > current else 0\n",
    "    # if the predicted future price is lower than the current price,\n",
    "    # then subtract the true future price from the current price\n",
    "    sell_profit = lambda current, true_future, pred_future: current - true_future if pred_future < current else 0\n",
    "    X_test = data[\"X_test\"]\n",
    "    y_test = data[\"y_test\"]\n",
    "    # perform prediction and get prices\n",
    "    y_pred = model.predict(X_test)\n",
    "    if SCALE:\n",
    "        y_test = np.squeeze(data[\"column_scaler\"][\"adjclose\"].inverse_transform(np.expand_dims(y_test, axis=0)))\n",
    "        y_pred = np.squeeze(data[\"column_scaler\"][\"adjclose\"].inverse_transform(y_pred))\n",
    "    test_df = data[\"test_df\"]\n",
    "    # add predicted future prices to the dataframe\n",
    "    test_df[f\"adjclose_{LOOKUP_STEP}\"] = y_pred\n",
    "    # add true future prices to the dataframe\n",
    "    test_df[f\"true_adjclose_{LOOKUP_STEP}\"] = y_test\n",
    "    # sort the dataframe by date\n",
    "    test_df.sort_index(inplace=True)\n",
    "    final_df = test_df\n",
    "    # add the buy profit column\n",
    "    final_df[\"buy_profit\"] = list(map(buy_profit, \n",
    "                                    final_df[\"adjclose\"], \n",
    "                                    final_df[f\"adjclose_{LOOKUP_STEP}\"], \n",
    "                                    final_df[f\"true_adjclose_{LOOKUP_STEP}\"])\n",
    "                                    # since we don't have profit for last sequence, add 0's\n",
    "                                    )\n",
    "    # add the sell profit column\n",
    "    final_df[\"sell_profit\"] = list(map(sell_profit, \n",
    "                                    final_df[\"adjclose\"], \n",
    "                                    final_df[f\"adjclose_{LOOKUP_STEP}\"], \n",
    "                                    final_df[f\"true_adjclose_{LOOKUP_STEP}\"])\n",
    "                                    # since we don't have profit for last sequence, add 0's\n",
    "                                    )\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, data):\n",
    "    # retrieve the last sequence from data\n",
    "    last_sequence = data[\"last_sequence\"][-N_STEPS:]\n",
    "    # expand dimension\n",
    "    last_sequence = np.expand_dims(last_sequence, axis=0)\n",
    "    # get the prediction (scaled from 0 to 1)\n",
    "    prediction = model.predict(last_sequence)\n",
    "    # get the price (by inverting the scaling)\n",
    "    if SCALE:\n",
    "        predicted_price = data[\"column_scaler\"][\"adjclose\"].inverse_transform(prediction)[0][0]\n",
    "    else:\n",
    "        predicted_price = prediction[0][0]\n",
    "    return predicted_price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load optimal model weights from results folder\n",
    "model_path = os.path.join(\"results\", model_name) + \".h5\"\n",
    "model.load_weights(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model\n",
    "loss, mae = model.evaluate(data[\"X_test\"], data[\"y_test\"], verbose=0)\n",
    "# calculate the mean absolute error (inverse scaling)\n",
    "if SCALE:\n",
    "    mean_absolute_error = data[\"column_scaler\"][\"adjclose\"].inverse_transform([[mae]])[0][0]\n",
    "else:\n",
    "    mean_absolute_error = mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the final dataframe for the testing set\n",
    "final_df = get_final_df(model, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict the future price\n",
    "future_price = predict(model, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we calculate the accuracy by counting the number of positive profits\n",
    "accuracy_score = (len(final_df[final_df['sell_profit'] > 0]) + len(final_df[final_df['buy_profit'] > 0])) / len(final_df)\n",
    "# calculating total buy & sell profit\n",
    "total_buy_profit  = final_df[\"buy_profit\"].sum()\n",
    "total_sell_profit = final_df[\"sell_profit\"].sum()\n",
    "# total profit by adding sell & buy together\n",
    "total_profit = total_buy_profit + total_sell_profit\n",
    "# dividing total profit by number of testing samples (number of trades)\n",
    "profit_per_trade = total_profit / len(final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Future price after 15 days is 3206.62$\n",
      "huber_loss loss: 0.00010393166303401813\n",
      "Mean Absolute Error: 24.49004207402624\n",
      "Accuracy score: 0.5392405063291139\n",
      "Total buy profit: 8544.1239554286\n",
      "Total sell profit: -5014.326686382294\n",
      "Total profit: 3529.7972690463066\n",
      "Profit per trade: 2.978731872612917\n"
     ]
    }
   ],
   "source": [
    "# printing metrics\n",
    "print(f\"Future price after {LOOKUP_STEP} days is {future_price:.2f}$\")\n",
    "print(f\"{LOSS} loss:\", loss)\n",
    "print(\"Mean Absolute Error:\", mean_absolute_error)\n",
    "print(\"Accuracy score:\", accuracy_score)\n",
    "print(\"Total buy profit:\", total_buy_profit)\n",
    "print(\"Total sell profit:\", total_sell_profit)\n",
    "print(\"Total profit:\", total_profit)\n",
    "print(\"Profit per trade:\", profit_per_trade)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_graph(final_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
